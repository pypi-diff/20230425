# Comparing `tmp/thirdai-0.6.8-cp39-cp39-win_amd64.whl.zip` & `tmp/thirdai-0.6.9-cp39-cp39-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,66 +1,66 @@
-Zip file size: 3874231 bytes, number of entries: 64
--rw-rw-rw-  2.0 fat      136 b- defN 23-Apr-15 14:14 thirdai/bolt_v2.py
--rw-rw-rw-  2.0 fat      145 b- defN 23-Apr-15 14:14 thirdai/deployment.py
--rw-rw-rw-  2.0 fat      139 b- defN 23-Apr-15 14:14 thirdai/distributed_bolt.py
--rw-rw-rw-  2.0 fat     1312 b- defN 23-Apr-15 14:14 thirdai/embeddings.py
--rw-rw-rw-  2.0 fat      136 b- defN 23-Apr-15 14:14 thirdai/hashing.py
--rw-rw-rw-  2.0 fat      195 b- defN 23-Apr-15 14:14 thirdai/licensing.py
--rw-rw-rw-  2.0 fat     1277 b- defN 23-Apr-15 14:14 thirdai/search.py
--rw-rw-rw-  2.0 fat     1624 b- defN 23-Apr-15 14:14 thirdai/_download.py
--rw-rw-rw-  2.0 fat  9388544 b- defN 23-Apr-15 14:14 thirdai/_thirdai.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat     2759 b- defN 23-Apr-15 14:14 thirdai/__init__.py
--rw-rw-rw-  2.0 fat     9515 b- defN 23-Apr-15 14:14 thirdai/bolt/udt_docs.py
--rw-rw-rw-  2.0 fat     4781 b- defN 23-Apr-15 14:14 thirdai/bolt/udt_modifications.py
--rw-rw-rw-  2.0 fat      226 b- defN 23-Apr-15 14:14 thirdai/bolt/__init__.py
--rw-rw-rw-  2.0 fat     1962 b- defN 23-Apr-15 14:14 thirdai/data/column_map_utils.py
--rw-rw-rw-  2.0 fat     2474 b- defN 23-Apr-15 14:14 thirdai/data/get_udt_columns.py
--rw-rw-rw-  2.0 fat     5748 b- defN 23-Apr-15 14:14 thirdai/data/type_inference.py
--rw-rw-rw-  2.0 fat      525 b- defN 23-Apr-15 14:14 thirdai/data/__init__.py
--rw-rw-rw-  2.0 fat     3784 b- defN 23-Apr-15 14:14 thirdai/dataset/csv_data_source.py
--rw-rw-rw-  2.0 fat     2061 b- defN 23-Apr-15 14:14 thirdai/dataset/parquet_data_source.py
--rw-rw-rw-  2.0 fat      245 b- defN 23-Apr-15 14:14 thirdai/dataset/__init__.py
--rw-rw-rw-  2.0 fat     1619 b- defN 23-Apr-15 14:14 thirdai/demos/beir_download_utils.py
--rw-rw-rw-  2.0 fat    24074 b- defN 23-Apr-15 14:14 thirdai/demos/download_datasets.py
--rw-rw-rw-  2.0 fat       34 b- defN 23-Apr-15 14:14 thirdai/demos/__init__.py
--rw-rw-rw-  2.0 fat     5263 b- defN 23-Apr-15 14:14 thirdai/telemetry/telemetry_daemon.py
--rw-rw-rw-  2.0 fat     5939 b- defN 23-Apr-15 14:14 thirdai/telemetry/telemetry_start_and_stop.py
--rw-rw-rw-  2.0 fat       51 b- defN 23-Apr-15 14:14 thirdai/telemetry/__init__.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Apr-15 14:14 thirdai/_deps/__init__.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Apr-15 14:14 thirdai/_deps/ColBERT/__init__.py
--rw-rw-rw-  2.0 fat     2118 b- defN 23-Apr-15 14:14 thirdai/_deps/ColBERT/colbertconfig/base_config.py
--rw-rw-rw-  2.0 fat      314 b- defN 23-Apr-15 14:14 thirdai/_deps/ColBERT/colbertconfig/config.py
--rw-rw-rw-  2.0 fat     2321 b- defN 23-Apr-15 14:14 thirdai/_deps/ColBERT/colbertconfig/core_config.py
--rw-rw-rw-  2.0 fat     3887 b- defN 23-Apr-15 14:14 thirdai/_deps/ColBERT/colbertconfig/settings.py
--rw-rw-rw-  2.0 fat       48 b- defN 23-Apr-15 14:14 thirdai/_deps/ColBERT/colbertconfig/__init__.py
--rw-rw-rw-  2.0 fat     1268 b- defN 23-Apr-15 14:14 thirdai/_deps/ColBERT/colbertmodeling/base_colbert.py
--rw-rw-rw-  2.0 fat     1418 b- defN 23-Apr-15 14:14 thirdai/_deps/ColBERT/colbertmodeling/checkpoint.py
--rw-rw-rw-  2.0 fat     2277 b- defN 23-Apr-15 14:14 thirdai/_deps/ColBERT/colbertmodeling/colbert.py
--rw-rw-rw-  2.0 fat     2175 b- defN 23-Apr-15 14:14 thirdai/_deps/ColBERT/colbertmodeling/hf_colbert.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Apr-15 14:14 thirdai/_deps/ColBERT/colbertmodeling/__init__.py
--rw-rw-rw-  2.0 fat     2064 b- defN 23-Apr-15 14:14 thirdai/_deps/ColBERT/colbertmodeling/tokenization/doc_tokenization.py
--rw-rw-rw-  2.0 fat     3295 b- defN 23-Apr-15 14:14 thirdai/_deps/ColBERT/colbertmodeling/tokenization/query_tokenization.py
--rw-rw-rw-  2.0 fat      168 b- defN 23-Apr-15 14:14 thirdai/_deps/ColBERT/colbertmodeling/tokenization/__init__.py
--rw-rw-rw-  2.0 fat      319 b- defN 23-Apr-15 14:14 thirdai/_deps/ColBERT/colbertutils/utils.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Apr-15 14:14 thirdai/_deps/ColBERT/colbertutils/__init__.py
--rw-rw-rw-  2.0 fat     9277 b- defN 23-Apr-15 14:14 thirdai/_distributed_bolt/dataset_loaders.py
--rw-rw-rw-  2.0 fat    25565 b- defN 23-Apr-15 14:14 thirdai/_distributed_bolt/distributed.py
--rw-rw-rw-  2.0 fat     3070 b- defN 23-Apr-15 14:14 thirdai/_distributed_bolt/utils.py
--rw-rw-rw-  2.0 fat      380 b- defN 23-Apr-15 14:14 thirdai/_distributed_bolt/__init__.py
--rw-rw-rw-  2.0 fat     3682 b- defN 23-Apr-15 14:14 thirdai/_distributed_bolt/backend/primary_worker.py
--rw-rw-rw-  2.0 fat     2117 b- defN 23-Apr-15 14:14 thirdai/_distributed_bolt/backend/replica_worker.py
--rw-rw-rw-  2.0 fat     8078 b- defN 23-Apr-15 14:14 thirdai/_distributed_bolt/backend/train_state_manager.py
--rw-rw-rw-  2.0 fat    10104 b- defN 23-Apr-15 14:14 thirdai/_distributed_bolt/backend/worker.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Apr-15 14:14 thirdai/_distributed_bolt/backend/__init__.py
--rw-rw-rw-  2.0 fat     6770 b- defN 23-Apr-15 14:14 thirdai/_distributed_bolt/backend/communication/circular.py
--rw-rw-rw-  2.0 fat     1205 b- defN 23-Apr-15 14:14 thirdai/_distributed_bolt/backend/communication/gloo.py
--rw-rw-rw-  2.0 fat     1297 b- defN 23-Apr-15 14:14 thirdai/_distributed_bolt/backend/communication/linear.py
--rw-rw-rw-  2.0 fat      162 b- defN 23-Apr-15 14:14 thirdai/_distributed_bolt/backend/communication/__init__.py
--rw-rw-rw-  2.0 fat      394 b- defN 23-Apr-15 14:14 thirdai-0.6.8.dist-info/DELVEWHEEL
--rw-rw-rw-  2.0 fat    16919 b- defN 23-Apr-15 14:14 thirdai-0.6.8.dist-info/LICENSE.txt
--rw-rw-rw-  2.0 fat     1982 b- defN 23-Apr-15 14:14 thirdai-0.6.8.dist-info/METADATA
--rw-rw-rw-  2.0 fat     6055 b- defN 23-Apr-15 14:14 thirdai-0.6.8.dist-info/RECORD
--rw-rw-rw-  2.0 fat        8 b- defN 23-Apr-15 14:14 thirdai-0.6.8.dist-info/top_level.txt
--rw-rw-rw-  2.0 fat      100 b- defN 23-Apr-15 14:14 thirdai-0.6.8.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       55 b- defN 23-Apr-15 14:14 thirdai.libs/.load-order-thirdai-0.6.8
+Zip file size: 3910547 bytes, number of entries: 64
+-rw-rw-rw-  2.0 fat      136 b- defN 23-Apr-25 00:35 thirdai/bolt_v2.py
+-rw-rw-rw-  2.0 fat      145 b- defN 23-Apr-25 00:35 thirdai/deployment.py
+-rw-rw-rw-  2.0 fat      139 b- defN 23-Apr-25 00:35 thirdai/distributed_bolt.py
+-rw-rw-rw-  2.0 fat     1312 b- defN 23-Apr-25 00:35 thirdai/embeddings.py
+-rw-rw-rw-  2.0 fat      136 b- defN 23-Apr-25 00:35 thirdai/hashing.py
+-rw-rw-rw-  2.0 fat      195 b- defN 23-Apr-25 00:35 thirdai/licensing.py
+-rw-rw-rw-  2.0 fat     1277 b- defN 23-Apr-25 00:35 thirdai/search.py
+-rw-rw-rw-  2.0 fat     1308 b- defN 23-Apr-25 00:35 thirdai/_download.py
+-rw-rw-rw-  2.0 fat  9515008 b- defN 23-Apr-25 00:35 thirdai/_thirdai.cp39-win_amd64.pyd
+-rw-rw-rw-  2.0 fat     2759 b- defN 23-Apr-25 00:35 thirdai/__init__.py
+-rw-rw-rw-  2.0 fat     9515 b- defN 23-Apr-25 00:35 thirdai/bolt/udt_docs.py
+-rw-rw-rw-  2.0 fat     5267 b- defN 23-Apr-25 00:35 thirdai/bolt/udt_modifications.py
+-rw-rw-rw-  2.0 fat      262 b- defN 23-Apr-25 00:35 thirdai/bolt/__init__.py
+-rw-rw-rw-  2.0 fat     1962 b- defN 23-Apr-25 00:35 thirdai/data/column_map_utils.py
+-rw-rw-rw-  2.0 fat     2474 b- defN 23-Apr-25 00:35 thirdai/data/get_udt_columns.py
+-rw-rw-rw-  2.0 fat     5748 b- defN 23-Apr-25 00:35 thirdai/data/type_inference.py
+-rw-rw-rw-  2.0 fat      525 b- defN 23-Apr-25 00:35 thirdai/data/__init__.py
+-rw-rw-rw-  2.0 fat     3784 b- defN 23-Apr-25 00:35 thirdai/dataset/csv_data_source.py
+-rw-rw-rw-  2.0 fat     2061 b- defN 23-Apr-25 00:35 thirdai/dataset/parquet_data_source.py
+-rw-rw-rw-  2.0 fat      245 b- defN 23-Apr-25 00:35 thirdai/dataset/__init__.py
+-rw-rw-rw-  2.0 fat     1619 b- defN 23-Apr-25 00:35 thirdai/demos/beir_download_utils.py
+-rw-rw-rw-  2.0 fat    24074 b- defN 23-Apr-25 00:35 thirdai/demos/download_datasets.py
+-rw-rw-rw-  2.0 fat       34 b- defN 23-Apr-25 00:35 thirdai/demos/__init__.py
+-rw-rw-rw-  2.0 fat     5263 b- defN 23-Apr-25 00:35 thirdai/telemetry/telemetry_daemon.py
+-rw-rw-rw-  2.0 fat     5939 b- defN 23-Apr-25 00:35 thirdai/telemetry/telemetry_start_and_stop.py
+-rw-rw-rw-  2.0 fat       51 b- defN 23-Apr-25 00:35 thirdai/telemetry/__init__.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Apr-25 00:35 thirdai/_deps/__init__.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Apr-25 00:35 thirdai/_deps/ColBERT/__init__.py
+-rw-rw-rw-  2.0 fat     2118 b- defN 23-Apr-25 00:35 thirdai/_deps/ColBERT/colbertconfig/base_config.py
+-rw-rw-rw-  2.0 fat      314 b- defN 23-Apr-25 00:35 thirdai/_deps/ColBERT/colbertconfig/config.py
+-rw-rw-rw-  2.0 fat     2321 b- defN 23-Apr-25 00:35 thirdai/_deps/ColBERT/colbertconfig/core_config.py
+-rw-rw-rw-  2.0 fat     3887 b- defN 23-Apr-25 00:35 thirdai/_deps/ColBERT/colbertconfig/settings.py
+-rw-rw-rw-  2.0 fat       48 b- defN 23-Apr-25 00:35 thirdai/_deps/ColBERT/colbertconfig/__init__.py
+-rw-rw-rw-  2.0 fat     1268 b- defN 23-Apr-25 00:35 thirdai/_deps/ColBERT/colbertmodeling/base_colbert.py
+-rw-rw-rw-  2.0 fat     1418 b- defN 23-Apr-25 00:35 thirdai/_deps/ColBERT/colbertmodeling/checkpoint.py
+-rw-rw-rw-  2.0 fat     2277 b- defN 23-Apr-25 00:35 thirdai/_deps/ColBERT/colbertmodeling/colbert.py
+-rw-rw-rw-  2.0 fat     2175 b- defN 23-Apr-25 00:35 thirdai/_deps/ColBERT/colbertmodeling/hf_colbert.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Apr-25 00:35 thirdai/_deps/ColBERT/colbertmodeling/__init__.py
+-rw-rw-rw-  2.0 fat     2064 b- defN 23-Apr-25 00:35 thirdai/_deps/ColBERT/colbertmodeling/tokenization/doc_tokenization.py
+-rw-rw-rw-  2.0 fat     3295 b- defN 23-Apr-25 00:35 thirdai/_deps/ColBERT/colbertmodeling/tokenization/query_tokenization.py
+-rw-rw-rw-  2.0 fat      168 b- defN 23-Apr-25 00:35 thirdai/_deps/ColBERT/colbertmodeling/tokenization/__init__.py
+-rw-rw-rw-  2.0 fat      319 b- defN 23-Apr-25 00:35 thirdai/_deps/ColBERT/colbertutils/utils.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Apr-25 00:35 thirdai/_deps/ColBERT/colbertutils/__init__.py
+-rw-rw-rw-  2.0 fat    10677 b- defN 23-Apr-25 00:35 thirdai/_distributed_bolt/dataset_loaders.py
+-rw-rw-rw-  2.0 fat    25788 b- defN 23-Apr-25 00:35 thirdai/_distributed_bolt/distributed.py
+-rw-rw-rw-  2.0 fat     3070 b- defN 23-Apr-25 00:35 thirdai/_distributed_bolt/utils.py
+-rw-rw-rw-  2.0 fat      454 b- defN 23-Apr-25 00:35 thirdai/_distributed_bolt/__init__.py
+-rw-rw-rw-  2.0 fat     3682 b- defN 23-Apr-25 00:35 thirdai/_distributed_bolt/backend/primary_worker.py
+-rw-rw-rw-  2.0 fat     2117 b- defN 23-Apr-25 00:35 thirdai/_distributed_bolt/backend/replica_worker.py
+-rw-rw-rw-  2.0 fat     8375 b- defN 23-Apr-25 00:35 thirdai/_distributed_bolt/backend/train_state_manager.py
+-rw-rw-rw-  2.0 fat    10214 b- defN 23-Apr-25 00:35 thirdai/_distributed_bolt/backend/worker.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Apr-25 00:35 thirdai/_distributed_bolt/backend/__init__.py
+-rw-rw-rw-  2.0 fat     6770 b- defN 23-Apr-25 00:35 thirdai/_distributed_bolt/backend/communication/circular.py
+-rw-rw-rw-  2.0 fat     1205 b- defN 23-Apr-25 00:35 thirdai/_distributed_bolt/backend/communication/gloo.py
+-rw-rw-rw-  2.0 fat     1297 b- defN 23-Apr-25 00:35 thirdai/_distributed_bolt/backend/communication/linear.py
+-rw-rw-rw-  2.0 fat      162 b- defN 23-Apr-25 00:35 thirdai/_distributed_bolt/backend/communication/__init__.py
+-rw-rw-rw-  2.0 fat      394 b- defN 23-Apr-25 00:35 thirdai-0.6.9.dist-info/DELVEWHEEL
+-rw-rw-rw-  2.0 fat    16919 b- defN 23-Apr-25 00:35 thirdai-0.6.9.dist-info/LICENSE.txt
+-rw-rw-rw-  2.0 fat     2006 b- defN 23-Apr-25 00:35 thirdai-0.6.9.dist-info/METADATA
+-rw-rw-rw-  2.0 fat     6056 b- defN 23-Apr-25 00:35 thirdai-0.6.9.dist-info/RECORD
+-rw-rw-rw-  2.0 fat        8 b- defN 23-Apr-25 00:35 thirdai-0.6.9.dist-info/top_level.txt
+-rw-rw-rw-  2.0 fat      100 b- defN 23-Apr-25 00:35 thirdai-0.6.9.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       54 b- defN 23-Apr-25 00:35 thirdai.libs/.load-order-thirdai-0.6.9
 -rw-rw-rw-  2.0 fat   698816 b- defN 22-Dec-13 08:41 thirdai.libs/libomp140.x86_64-1bc3435a2096f922cfee744b10f0b66d.dll
-64 files, 10282276 bytes uncompressed, 3864359 bytes compressed:  62.4%
+64 files, 10411074 bytes uncompressed, 3900675 bytes compressed:  62.5%
```

## zipnote {}

```diff
@@ -162,32 +162,32 @@
 
 Filename: thirdai/_distributed_bolt/backend/communication/linear.py
 Comment: 
 
 Filename: thirdai/_distributed_bolt/backend/communication/__init__.py
 Comment: 
 
-Filename: thirdai-0.6.8.dist-info/DELVEWHEEL
+Filename: thirdai-0.6.9.dist-info/DELVEWHEEL
 Comment: 
 
-Filename: thirdai-0.6.8.dist-info/LICENSE.txt
+Filename: thirdai-0.6.9.dist-info/LICENSE.txt
 Comment: 
 
-Filename: thirdai-0.6.8.dist-info/METADATA
+Filename: thirdai-0.6.9.dist-info/METADATA
 Comment: 
 
-Filename: thirdai-0.6.8.dist-info/RECORD
+Filename: thirdai-0.6.9.dist-info/RECORD
 Comment: 
 
-Filename: thirdai-0.6.8.dist-info/top_level.txt
+Filename: thirdai-0.6.9.dist-info/top_level.txt
 Comment: 
 
-Filename: thirdai-0.6.8.dist-info/WHEEL
+Filename: thirdai-0.6.9.dist-info/WHEEL
 Comment: 
 
-Filename: thirdai.libs/.load-order-thirdai-0.6.8
+Filename: thirdai.libs/.load-order-thirdai-0.6.9
 Comment: 
 
 Filename: thirdai.libs/libomp140.x86_64-1bc3435a2096f922cfee744b10f0b66d.dll
 Comment: 
 
 Zip file comment:
```

## thirdai/_download.py

```diff
@@ -1,18 +1,13 @@
 import os
 import pathlib
 
 CACHE_DIR = pathlib.Path.home() / ".cache" / "thirdai"
 
 
-# TODO(josh): These methods aren't robust (they relies on the user having curl
-# and tar working on their machine), but the python requests library wasn't
-# working to download from dropbox so I am just going with this for now.
-# We should make this a more robust solution like huggingface's dataset
-# library.
 def _unzip_targz(targz_path, destination_parent_folder):
     os.system(f"tar -xzf {targz_path} -C {destination_parent_folder}")
 
 
 def _download_file(url, download_path):
     os.system(f"curl -L {url} -o {download_path}")
```

## thirdai/__init__.py

```diff
@@ -1,35 +1,35 @@
 """The ThirdAI Python package"""
 
 
 # start delvewheel patch
-def _delvewheel_init_patch_1_3_5():
+def _delvewheel_init_patch_1_3_6():
     import ctypes
     import os
     import platform
     import sys
     libs_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir, 'thirdai.libs'))
     is_pyinstaller = getattr(sys, 'frozen', False) and hasattr(sys, '_MEIPASS')
     is_conda_cpython = platform.python_implementation() == 'CPython' and (hasattr(ctypes.pythonapi, 'Anaconda_GetVersion') or 'packaged by conda-forge' in sys.version)
     if sys.version_info[:2] >= (3, 8) and not is_conda_cpython or sys.version_info[:2] >= (3, 10):
         if not is_pyinstaller or os.path.isdir(libs_dir):
             os.add_dll_directory(libs_dir)
     else:
-        load_order_filepath = os.path.join(libs_dir, '.load-order-thirdai-0.6.8')
+        load_order_filepath = os.path.join(libs_dir, '.load-order-thirdai-0.6.9')
         if not is_pyinstaller or os.path.isfile(load_order_filepath):
-            with open(os.path.join(libs_dir, '.load-order-thirdai-0.6.8')) as file:
+            with open(os.path.join(libs_dir, '.load-order-thirdai-0.6.9')) as file:
                 load_order = file.read().split()
             for lib in load_order:
                 lib_path = os.path.join(os.path.join(libs_dir, lib))
                 if not is_pyinstaller or os.path.isfile(lib_path):
                     ctypes.WinDLL(lib_path)
 
 
-_delvewheel_init_patch_1_3_5()
-del _delvewheel_init_patch_1_3_5
+_delvewheel_init_patch_1_3_6()
+del _delvewheel_init_patch_1_3_6
 # end delvewheel patch
 
 
 __all__ = [
     "bolt",
     "search",
     "dataset",
```

## thirdai/bolt/udt_modifications.py

```diff
@@ -131,21 +131,38 @@
     delattr(bolt.UDT, "cold_start")
 
     bolt.UDT.train = wrapped_train
     bolt.UDT.evaluate = wrapped_evaluate
     bolt.UDT.cold_start = wrapped_cold_start
 
 
+def modify_mach_udt():
+    original_introduce_documents = bolt.UDT.introduce_documents
+
+    def wrapped_introduce_documents(
+        self,
+        filename: str,
+        strong_column_names: List[str],
+        weak_column_names: List[str],
+    ):
+        data_source = _create_data_source(filename)
+
+        return original_introduce_documents(
+            self, data_source, strong_column_names, weak_column_names
+        )
+
+    delattr(bolt.UDT, "introduce_documents")
+
+    bolt.UDT.introduce_documents = wrapped_introduce_documents
+
+
 def modify_graph_udt():
     original_index_nodes_method = bolt.UDT.index_nodes
 
     def wrapped_index_nodes(self, filename: str):
         data_source = _create_data_source(filename)
 
         original_index_nodes_method(self, data_source)
 
-    # TODO(Josh)
-    # wrapped_index.__doc__ = udt_graph_index_doc
-
     delattr(bolt.UDT, "index_nodes")
 
     bolt.UDT.index_nodes = wrapped_index_nodes
```

## thirdai/bolt/__init__.py

```diff
@@ -1,10 +1,11 @@
 import thirdai._thirdai.bolt
 from thirdai._thirdai.bolt import *
 
-from .udt_modifications import modify_graph_udt, modify_udt
+from .udt_modifications import modify_graph_udt, modify_mach_udt, modify_udt
 
 modify_udt()
 modify_graph_udt()
+modify_mach_udt()
 
 __all__ = []
 __all__.extend(dir(thirdai._thirdai.bolt))
```

## thirdai/_distributed_bolt/dataset_loaders.py

```diff
@@ -1,16 +1,14 @@
 from abc import ABC, abstractmethod
 from dataclasses import dataclass
 from typing import Callable, List, Optional, Tuple, Union
 
 from thirdai import bolt, data, dataset
 from thirdai.bolt.udt_modifications import _create_data_source
 
-# TODO(Josh/Pratik): Clean up this file and remove the unnecessary DatasetLoaders
-
 
 class DistributedDatasetLoader(ABC):
     @abstractmethod
     def next() -> Optional[List[dataset.BoltDataset]]:
         """
         This function returns training data and labels if there is training data left for
         ingestion for a epoch else, will return NULL.
@@ -49,47 +47,66 @@
     def __init__(
         self,
         batch_size,
         data_source_factory,
         max_in_memory_batches=None,
         featurizer=None,
         shuffle=True,
+        with_prompt=True,
+        batches_to_skip=0,
+        min_vecs_in_buffer=64000,
         *args,
         **kwargs,
     ):
         self.featurizer = featurizer
         self.batch_size = batch_size
         self.max_in_memory_batches = max_in_memory_batches
         self.shuffle = shuffle
         self.data_source_factory = data_source_factory
+        self.with_prompt = with_prompt
+        self.batches_to_skip = batches_to_skip
+        self.min_vecs_in_buffer = min_vecs_in_buffer
         self.args = args
         self.kwargs = kwargs
         self.dataset_finished = False
 
     def load(self):
         data_source = self.data_source_factory(*self.args, **self.kwargs)
         self.generator = dataset.DatasetLoader(
             data_source=data_source,
             featurizer=self.featurizer,
             shuffle=self.shuffle,
+            shuffle_config=dataset.ShuffleConfig(
+                min_vecs_in_buffer=self.min_vecs_in_buffer
+            ),
         )
+        # Note(pratik): This would still be approximate. Since, seed for buffer
+        # shuffling would be different for each run.
+        while self.batches_to_skip > 0:
+            num_batches_to_load = min(self.batches_to_skip, self.max_in_memory_batches)
+            self.generator.load_some(
+                num_batches=num_batches_to_load, batch_size=self.batch_size
+            )
+            self.batches_to_skip -= num_batches_to_load
 
     def next(self):
         if self.dataset_finished:
             return None
 
         if self.max_in_memory_batches == None:
             load = self.generator.load_all(batch_size=self.batch_size)
             self.dataset_finished = True
         else:
             load = self.generator.load_some(
                 num_batches=self.max_in_memory_batches, batch_size=self.batch_size
             )
+        if self.with_prompt:
+            return load
 
-        return load
+        return load[1:]
 
     def restart(self):
         self.generator.restart()
 
 
 @dataclass
 class ValidationContext:
@@ -101,27 +118,34 @@
 
 class DistributedUDTDatasetLoader(DistributedDatasetLoader):
     def __init__(
         self,
         train_file: str,
         batch_size: int,
         data_processor,
+        min_vecs_in_buffer=None,
         max_in_memory_batches: int = None,
     ):
         self.generator = None
         self.data_processor = data_processor
         self.train_file = train_file
         self.batch_size = batch_size
         self.max_in_memory_batches = max_in_memory_batches
         self.dataset_finished = False
+        self.min_vecs_in_buffer = min_vecs_in_buffer
 
     def load(self, shuffle: bool = True):
         self.generator = self.data_processor.get_dataset_loader(
             _create_data_source(self.train_file),
             training=shuffle,
+            shuffle_config=(
+                dataset.ShuffleConfig(min_vecs_in_buffer=self.min_vecs_in_buffer)
+                if self.min_vecs_in_buffer is not None
+                else None
+            ),
         )
 
     def next(self):
         if self.dataset_finished:
             return None
 
         if self.max_in_memory_batches == None:
@@ -145,38 +169,46 @@
         train_file: str,
         batch_size: int,
         max_in_memory_batches: int,
         strong_column_names: List[str],
         weak_column_names: List[str],
         data_processor,
         cold_start_meta_data,
+        min_vecs_in_buffer=None,
     ):
         self.generator = None
         self.train_file = train_file
         self.strong_column_names = strong_column_names
         self.weak_column_names = weak_column_names
         self.batch_size = batch_size
         self.max_in_memory_batches = max_in_memory_batches
         self.dataset_finished = False
         self.data_processor = data_processor
         self.cold_start_meta_data = cold_start_meta_data
+        self.min_vecs_in_buffer = min_vecs_in_buffer
 
     def load(self, shuffle: bool = True):
         original_data_source = _create_data_source(self.train_file)
         cold_start_data_source = (
             bolt.distributed_preprocessing.preprocess_cold_start_train_source(
                 original_data_source,
                 self.strong_column_names,
                 self.weak_column_names,
                 self.data_processor,
                 self.cold_start_meta_data,
             )
         )
         self.generator = self.data_processor.get_dataset_loader(
-            cold_start_data_source, training=shuffle
+            cold_start_data_source,
+            training=shuffle,
+            shuffle_config=(
+                dataset.ShuffleConfig(min_vecs_in_buffer=self.min_vecs_in_buffer)
+                if self.min_vecs_in_buffer is not None
+                else None
+            ),
         )
 
 
 class DistributedGenericInMemoryDatasetLoader(DistributedDatasetLoader):
     """
     Wraps a generator function that returns a single pair of training and label
     datasets into an in memory data generator ready to pass into the distributed
```

## thirdai/_distributed_bolt/distributed.py

```diff
@@ -13,15 +13,15 @@
 from thirdai._distributed_bolt.backend.train_state_manager import TrainStateManager
 from thirdai._distributed_bolt.dataset_loaders import (
     DistributedColdStartDatasetLoader,
     DistributedDatasetLoader,
     DistributedUDTDatasetLoader,
     ValidationContext,
 )
-from thirdai._thirdai import bolt
+from thirdai._thirdai import bolt, dataset
 
 from .utils import get_num_cpus, init_logging
 
 
 def add_distributed_to_udt():
     def batch_size_per_node(batch_size, cluster_config):
         if batch_size is None:
@@ -73,14 +73,15 @@
         batch_size: Optional[int] = None,
         learning_rate: float = 0.001,
         epochs: int = 3,
         max_in_memory_batches: Optional[int] = None,
         metrics: List[str] = [],
         verbose: bool = True,
         validation: Optional[bolt.Validation] = None,
+        min_vecs_in_buffer: Optional[int] = None,
     ):
         """
         This function trains UDT in the distributed setting. ThirdAI uses Ray
         Core(https://docs.ray.io/en/latest/ray-core/walkthrough.html) for its
         distributed offering. This function assumes there is a ray cluster already
         running on the machine where this function is called or the machine should
         have an access to a ray cluster.
@@ -133,14 +134,15 @@
 
         train_sources = [
             DistributedUDTDatasetLoader(
                 train_file=file,
                 batch_size=batch_size_per_node(batch_size, cluster_config),
                 max_in_memory_batches=max_in_memory_batches,
                 data_processor=self.get_data_processor(),
+                min_vecs_in_buffer=min_vecs_in_buffer,
             )
             for file in filenames
         ]
 
         validation_context = None
         if validation != None:
             validation_source = DistributedUDTDatasetLoader(
@@ -180,14 +182,15 @@
         max_in_memory_batches: Optional[int] = None,
         batch_size: Optional[int] = None,
         learning_rate: float = 0.001,
         epochs: int = 5,
         metrics: List[str] = [],
         verbose: bool = True,
         validation: Optional[bolt.Validation] = None,
+        min_vecs_in_buffer: Optional[int] = None,
     ):
         """
         This function does cold-start pretraining for UDT in the distributed setting.
         ThirdAI uses Ray Core(https://docs.ray.io/en/latest/ray-core/walkthrough.html) for its
         distributed offering. This function assumes there is a ray cluster already
         running on the machine where this function is called or the machine should
         have an access to a ray cluster.
@@ -262,14 +265,15 @@
                 train_file=file,
                 batch_size=batch_size_per_node(batch_size, cluster_config),
                 max_in_memory_batches=max_in_memory_batches,
                 strong_column_names=strong_column_names,
                 weak_column_names=weak_column_names,
                 data_processor=self.get_data_processor(),
                 cold_start_meta_data=self.get_cold_start_meta_data(),
+                min_vecs_in_buffer=min_vecs_in_buffer,
             )
             for file in filenames
         ]
 
         validation_context = None
         if validation != None:
             validation_source = DistributedColdStartDatasetLoader(
```

## thirdai/_distributed_bolt/__init__.py

```diff
@@ -1,11 +1,13 @@
 from .dataset_loaders import (
+    DistributedColdStartDatasetLoader,
     DistributedFeaturizerDatasetLoader,
     DistributedSvmDatasetLoader,
     DistributedTabularDatasetLoader,
+    DistributedUDTDatasetLoader,
     ValidationContext,
 )
 from .distributed import (
     DistributedDataParallel,
     RayTrainingClusterConfig,
     add_distributed_to_udt,
 )
```

## thirdai/_distributed_bolt/backend/train_state_manager.py

```diff
@@ -184,7 +184,16 @@
         """
         self.logging.info(
             f"Epoch No: {epoch}, Batch Count: {self.batch_id_within_epoch}, Bolt Computation Time: {self.bolt_computation_time}, Averaging and Communcation Time: {self.averaging_and_communication_time}"
         )
 
     def validate_and_save_if_best(self):
         return ray.get(self.workers[0].validate_and_save_if_best.remote())
+
+    def update_learning_rate(self, learning_rate):
+        self.logging.info(f"Updating learning_rate to {learning_rate}")
+        ray.get(
+            [
+                worker.update_learning_rate.remote(learning_rate)
+                for worker in self.workers
+            ]
+        )
```

## thirdai/_distributed_bolt/backend/worker.py

```diff
@@ -271,12 +271,15 @@
 
     def freeze_hash_tables(self):
         self.model.freeze_hash_tables(True)
 
     def get_updated_metrics(self):
         return self.model.get_updated_metrics()
 
+    def update_learning_rate(self, learning_rate):
+        self.model.update_learning_rate(learning_rate)
+
     def model(self, with_optimizer):
         # setting with_optimizer flag, here implies that model would be serialized/pickled with optimizer. It is similar to how save/checkpoint works as pickling also uses cereal.
         if with_optimizer:
             self.model.should_save_optimizer(True)
         return self.model.model
```

## Comparing `thirdai-0.6.8.dist-info/LICENSE.txt` & `thirdai-0.6.9.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `thirdai-0.6.8.dist-info/METADATA` & `thirdai-0.6.9.dist-info/METADATA`

 * *Files 4% similar despite different names*

```diff
@@ -1,17 +1,18 @@
 Metadata-Version: 2.1
 Name: thirdai
-Version: 0.6.8
+Version: 0.6.9
 Summary: A faster cpu machine learning library
 Author: ThirdAI
 Author-email: contact@thirdai.com
 License: proprietary
 License-File: LICENSE.txt
 Requires-Dist: numpy
 Requires-Dist: typing-extensions
+Requires-Dist: requests
 Requires-Dist: pandas (>=1.2.0)
 Provides-Extra: benchmark
 Requires-Dist: toml ; extra == 'benchmark'
 Requires-Dist: psutil ; extra == 'benchmark'
 Requires-Dist: scikit-learn ; extra == 'benchmark'
 Requires-Dist: mlflow ; extra == 'benchmark'
 Requires-Dist: protobuf ; extra == 'benchmark'
```

## Comparing `thirdai-0.6.8.dist-info/RECORD` & `thirdai-0.6.9.dist-info/RECORD`

 * *Files 6% similar despite different names*

```diff
@@ -1,20 +1,20 @@
 thirdai/bolt_v2.py,sha256=2jdGmVwFGSR0AC9E91zCvDQgZtlc-_K9gYDgg4gn6ws,136
 thirdai/deployment.py,sha256=e_06MzSrth4S8Gs2dpEzfnLPaZsDcGhcJav8qfxEczY,145
 thirdai/distributed_bolt.py,sha256=MwUmwd8Q2v-ZAMtuGPBAocrQxO7ZRZZruWhlAuZzJos,139
 thirdai/embeddings.py,sha256=Afq5qcejw2NUpC_jImB5D70YLWB4ZHWROZPL7GdT44E,1312
 thirdai/hashing.py,sha256=MttqG8D0RnrPW2U-9Tf9LekMJjwnVS8I8jQAaNlucu4,136
 thirdai/licensing.py,sha256=oLmHWRqfqq6jpTTdauTyQHrY6LO_ukdTTdhgWotOPUQ,195
 thirdai/search.py,sha256=8HTl393Vt3g-OweibrR5UTYTCDCoF6Rk6WvWYRv30rM,1277
-thirdai/_download.py,sha256=RaYCBN4qjutcYNtoV286-GYnPX3ACq1C7aEmP_x1N2g,1624
-thirdai/_thirdai.cp39-win_amd64.pyd,sha256=zZyUeqZhYq6lPPZ8j1i9pQtS6j0eIs1zAux4WyE999M,9388544
-thirdai/__init__.py,sha256=2LmdjA2yv4S3EauRP4cCcjnvoXyDu5G1s02HFgQWmvE,2759
+thirdai/_download.py,sha256=16FZSiv34ZTi-u7qSM5V9pINi8Cke2qYoflLfs7YeEI,1308
+thirdai/_thirdai.cp39-win_amd64.pyd,sha256=PjZG-Mp8z5w_HiK3t7Iscq4OTkiiUc3-zbiZf_9M0RE,9515008
+thirdai/__init__.py,sha256=w-17JzJFadmZG2YeGsyS4DEpKKvQPSGty3Ivp7zzoCY,2759
 thirdai/bolt/udt_docs.py,sha256=jC40F00qaynfyBWnzY05OOM0r9EFZjzDzc_9ImHmWck,9515
-thirdai/bolt/udt_modifications.py,sha256=NXZNc4eUxPUSNiP5aATL_Z3hF1dwU4JCk-B6RzLpAD4,4781
-thirdai/bolt/__init__.py,sha256=GwbYQ4ho9kZcRWzvl1e1nPciVAZ1ABE--TlLSbyHk_s,226
+thirdai/bolt/udt_modifications.py,sha256=_z1biXuOZIfdpAsbRJHGObxcC21Q647_pjBlsxyOA-w,5267
+thirdai/bolt/__init__.py,sha256=I-_6s4PC8zPoCNDyw2KUKzsKgF8wPgHNnBTz1HnWJys,262
 thirdai/data/column_map_utils.py,sha256=cGvuxK7t2JOss25ImX28nMVnDjC0oXxSQBr5D2fdixU,1962
 thirdai/data/get_udt_columns.py,sha256=isYKw8MDHkGbmmnCfwbcS4d0RMaRhmbKtUKTZL2a93s,2474
 thirdai/data/type_inference.py,sha256=gzwd7eCEq6hfmzZRfm0XW99699FcsRXXYBAMTVsFLRQ,5748
 thirdai/data/__init__.py,sha256=ClztQDn5cxQRgjLEwIpep7_8EkXXTSStns3JpUlf2x0,525
 thirdai/dataset/csv_data_source.py,sha256=ZmYW3BsUPMZ5PMnO7ByP9HUnvcg2hkxK_WDEn1w7E5w,3784
 thirdai/dataset/parquet_data_source.py,sha256=hHVe7qgN7gWU1IU2u9P5Sb8F2ER_ZY-1ITPVpXWdAnI,2061
 thirdai/dataset/__init__.py,sha256=xxR8lQkYyH10KtSvyp4cpqQTMDocl-PlXJgrn6yUOmI,245
@@ -37,28 +37,28 @@
 thirdai/_deps/ColBERT/colbertmodeling/hf_colbert.py,sha256=80OcRzbGaeu6TRRwItrFtTrxqqBEwS4a_JLLwJjvQBU,2175
 thirdai/_deps/ColBERT/colbertmodeling/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 thirdai/_deps/ColBERT/colbertmodeling/tokenization/doc_tokenization.py,sha256=wTj2V-nAAludCX7V9dwaETvnwyBAKCtsYQ6pegxK--M,2064
 thirdai/_deps/ColBERT/colbertmodeling/tokenization/query_tokenization.py,sha256=pr4oYC__9BzE-eabPPoFOlz9X9PyVtDE8232Z3Bam3s,3295
 thirdai/_deps/ColBERT/colbertmodeling/tokenization/__init__.py,sha256=7Rwy_hl_63cv9GnOParV2deBNvY1HkC6lZQm76r--wE,168
 thirdai/_deps/ColBERT/colbertutils/utils.py,sha256=TsQ71Ufd5Bu5QC1wWxQPvnSAEwjft8KLq8mUgcugf7Q,319
 thirdai/_deps/ColBERT/colbertutils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-thirdai/_distributed_bolt/dataset_loaders.py,sha256=b6oK2ZIfOoxQb0RzlbmPZmvZUxs2IsMv2wYlUTTv0r8,9277
-thirdai/_distributed_bolt/distributed.py,sha256=kRZSZQo1VbWBKauK7mcJ32K2HugriBODeFFh3aO0yUI,25565
+thirdai/_distributed_bolt/dataset_loaders.py,sha256=vzWVnY8NH5Kc3ssS4ST6GyIXTfoTVF1E4l-GFazYcjQ,10677
+thirdai/_distributed_bolt/distributed.py,sha256=Kd5oT14dtRps3GPr_a_CEATCXV1qO5VcjAn4PlnT_-0,25788
 thirdai/_distributed_bolt/utils.py,sha256=1hCS1KXDulFxsmJkZUYE_VqM0-HOkCUfCe9i6gz1xME,3070
-thirdai/_distributed_bolt/__init__.py,sha256=oGQcLT4GLyml_0um5drl-mAYiS_MlhEEn6W-c-Dkkac,380
+thirdai/_distributed_bolt/__init__.py,sha256=cg4TLV_y1zkhw52mMPqd83PnNvnpEr0CUgmGRoBx4v8,454
 thirdai/_distributed_bolt/backend/primary_worker.py,sha256=pL_cYeWpXP1djCW0bo0G76F9_ptOtd23kjBgi9ZBumc,3682
 thirdai/_distributed_bolt/backend/replica_worker.py,sha256=ianxNsLKAsAkl3P4-6_GtWytVPPnxPOWINYul4Vse2o,2117
-thirdai/_distributed_bolt/backend/train_state_manager.py,sha256=hMXZiE61ygUZwwjfPD0gXHRw_cxWt4gcDRaig6-Ug34,8078
-thirdai/_distributed_bolt/backend/worker.py,sha256=r5JbgT4YjW2fyREYYs4cb51cq3_KU3djjm677LEywgs,10104
+thirdai/_distributed_bolt/backend/train_state_manager.py,sha256=i0Lhe5Zrp79YbRjmI7W9XFa3UYHvgdRup3Rpll9lYJs,8375
+thirdai/_distributed_bolt/backend/worker.py,sha256=GF_wPBdbo8QoJ8QaQVa_zCz7seaUHftwCRGVx3Pjg-c,10214
 thirdai/_distributed_bolt/backend/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 thirdai/_distributed_bolt/backend/communication/circular.py,sha256=ukeccwvzBQUe8eGJnlj7F50YkonFjnOQ-hcA19lo4hE,6770
 thirdai/_distributed_bolt/backend/communication/gloo.py,sha256=caO-6pSbwpblugfBeM2FEcynAnuzYcD4X96kdNnsnYg,1205
 thirdai/_distributed_bolt/backend/communication/linear.py,sha256=AzrQsUpl75YKYFsMB4R94a4UYARqGTSdg05guTxMDnM,1297
 thirdai/_distributed_bolt/backend/communication/__init__.py,sha256=HyoGy-fQvPpitxXjHPpJkUL6JmRgjGhMBALY6MUDUUk,162
-thirdai-0.6.8.dist-info/DELVEWHEEL,sha256=4Ui8YSERDl56kX2d4ruTKGWw6uBRu_tExbJECmCM1gc,394
-thirdai-0.6.8.dist-info/LICENSE.txt,sha256=agSIzKgTDMipzr2SjcOEG_GTJJkwtt-4wrcVAY03gPo,16919
-thirdai-0.6.8.dist-info/METADATA,sha256=NuBVhBcrgilCB3Yx8y94BV4brPZ2Fb0zH3FCFcaFstU,1982
-thirdai-0.6.8.dist-info/RECORD,,
-thirdai-0.6.8.dist-info/top_level.txt,sha256=yBixDeDldyBUN70Yfq9rnKJYCNICw3ae7uow_BdZysA,8
-thirdai-0.6.8.dist-info/WHEEL,sha256=fVcVlLzi8CGi_Ul8vjMdn8gER25dn5GBg9E6k9z41-Y,100
-thirdai.libs/.load-order-thirdai-0.6.8,sha256=PnB_pjt7USfiO_MLxC4SBntNtCRRaAfqNqjO3elkeDc,55
+thirdai-0.6.9.dist-info/DELVEWHEEL,sha256=CpXlKegaPHRdy0s9i3AHTPvNDzgSkbOOzEBcbvSEHDc,394
+thirdai-0.6.9.dist-info/LICENSE.txt,sha256=agSIzKgTDMipzr2SjcOEG_GTJJkwtt-4wrcVAY03gPo,16919
+thirdai-0.6.9.dist-info/METADATA,sha256=fRsQw4eI-F5dRCJ9jaLnaE2q_Cdd15Q64lnMZgnJmI4,2006
+thirdai-0.6.9.dist-info/RECORD,,
+thirdai-0.6.9.dist-info/top_level.txt,sha256=yBixDeDldyBUN70Yfq9rnKJYCNICw3ae7uow_BdZysA,8
+thirdai-0.6.9.dist-info/WHEEL,sha256=fVcVlLzi8CGi_Ul8vjMdn8gER25dn5GBg9E6k9z41-Y,100
+thirdai.libs/.load-order-thirdai-0.6.9,sha256=IHePHJ5PtHmPMdnvl1nS1UtrpLNXjldzSf53J817b7Q,54
 thirdai.libs/libomp140.x86_64-1bc3435a2096f922cfee744b10f0b66d.dll,sha256=HCSv3PjlY5GvNciweA8XnaDtQPgUndMvs6t5NpCFPB8,698816
```

